---
title: 腾讯广告部门一面-自然语言处理方向
categories: 腾讯广告一面
tags: 面试
mathjax: true
abbrlink: 6306
date: 2019-07-11 15:09:06
---
今天上午10点突然接到面试的电话，面完之后感觉不怎么好，还是总结一下吧。
## Pool的作用
池化层是一个采样的过程。
![Z24iUU.png](https://s2.ax1x.com/2019/07/11/Z24iUU.png)
![Z24QUO.png](https://s2.ax1x.com/2019/07/11/Z24QUO.png)
## Word2Vec的损失函数

## ElMo的损失函数
### 预训练部分
在 EMLo 中，他们使用的是一个双向的 LSTM 语言模型，由一个前向和一个后向语言
模型构成，目标函数就是取这两个方向语言模型的最大似然。
前向部分
$$
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{1}, t_{2}, \ldots, t_{k-1}\right)
$$
反向部分
$$
p\left(t_{1}, t_{2}, \ldots, t_{N}\right)=\prod_{k=1}^{N} p\left(t_{k} | t_{k+1}, t_{k+2}, \ldots, t_{N}\right)
$$
合起来部分如下，也就是损失函数
$$
\begin{array}{l}{\sum_{k=1}^{N}\left(\log p\left(t_{k} | t_{1}, \ldots, t_{k-1} ; \Theta_{x}, \vec{\Theta}_{LSTM}, 
\Theta_{s}\right)\right.} \\ {\quad+\log p\left(t_{k} | t_{k+1}, \ldots, t_{N} ; \Theta_{x}, \widetilde{\Theta}_{LSTM}, \Theta_{s}\right) )}\end{array}
$$
### 微调部分（用于下游任务）
在进行有监督的 NLP 任务时，可以将 ELMo 直接当做特征拼接到具体任务模型的
词向量输入，具体来说就是把这个双向语言模型的每一中间层进行一个求和得到动态的词向量表示。
提取词向量的过程如下：对于第K个Token,使用L层的双向ELMo可以得到的表示如下：
$$
\begin{aligned} R_{k} &=\left\{\mathbf{x}_{k}^{L M}, \overrightarrow{\mathbf{h}}_{k, j}^{L M}, \widetilde{\mathbf{h}}_{k, j}^{L M} | j=1, \ldots, L\right\} \\ &=\left\{\mathbf{h}_{k, j}^{L M} | j=0, \ldots, L\right\} \end{aligned}
$$
对于下游任务来说，得到表示就是各层双向LSTM的表示的加权和
$$
\mathbf{E} \mathbf{L} \mathbf{M} \mathbf{o}_{k}^{\text {task}}=E\left(R_{k} ; \Theta^{\text {task}}\right)=\gamma^{\text {task}} \sum_{j=0}^{L} s_{j}^{\text {task}} \mathbf{h}_{k, j}^{L M}
$$

## 快速排序的复杂度计算

