---
title: 论文阅读-20190924
tags: 论文
mathjax: true
abbrlink: 50252
date: 2019-09-24 20:45:28
---
## 论文1
**《SANVis: Visual Analytics for Understanding Self-Attention Networks》**
### 简介
这一篇文章是关于Self-Attention可视化的，可视化的网络有Transformer和BERT。里面有一幅Transformer的图很好，贴过来。

![](https://i.loli.net/2019/09/24/RF15kyJbirDtnh3.png)

<!--more-->


## 论文2
**《BERT Meets Chinese Word Segmentation》**
### 简介
这篇论文介绍BERT用于中文分词任务的。里面提到一点有趣的地方在于，BERT后面接Softmax和CRF层的最终效果差不多，但是SoftMax更快。
**模型结构**
![](https://i.loli.net/2019/09/24/nUMNzwXO9gPY71b.png)
**实验结果**
注意这个是只使用BERT的第一层做特征提取的效果，这里CRF要比SoftMax好。
![](https://i.loli.net/2019/09/24/J7HroBZNMEi5qyu.png)
这个是不同层做特征提取的效果，可以看到微调12层的时候SoftMax和CRF效果差不多。
![](https://i.loli.net/2019/09/24/er5Mnz1kPaS7lC4.png)

### 论文结果
- BERT可以稍微提高CWS任务的性能。就Softmax分类器来说，MSR数据集和PKU数据集F1分数分别有+0.3和+0.4提高。
- 充分训练的时候，CRF和Softmax达到相同的性能。但是由于Softmax预测时间更短，因此更受欢迎。
- 随着模型尺寸的增加，BERT的性能逐渐提高。

## 论文3 
**《Enriching BERT with Knowledge Graph Embeddings for Document Classification》**
### 简介
这篇文章是[一个比赛](https://competitions.codalab.org/competitions/20139#learn_the_details)的方法，这个比赛是进行文档分类，但是还有一些其他的数据（作者，ISBN等等），因此要做特征的融合，作者提出的融合方法很简单，如下：
![](https://i.loli.net/2019/09/24/ByvGczodnQZFjEq.png)

## 论文4
**《Subword ELMo》**
### 简介
这篇文章主要使用Subword提升ELMo的性能，思路很简单。这里主要看一下网络结构。
![](https://i.loli.net/2019/09/24/xrv4EGNKWqBlmkn.png)
稍微扩充一下Highway Network,[论文](https://arxiv.org/pdf/1505.00387.pdf),公式如下，其中$$T=sigmoid(wx + b)$$:
![](https://i.loli.net/2019/09/24/yYWfLEptgezoCAd.png)
**流程图**
![](https://i.loli.net/2019/09/24/VNnrUc96zJwMBG3.png)
**对比ResNet**
![](https://i.loli.net/2019/09/24/FUT4zA9lNufjob8.png)

